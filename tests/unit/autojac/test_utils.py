from pytest import mark, raises
from settings import DEVICE, DTYPE
from torch.nn import Linear, MSELoss, ReLU, Sequential
from utils.tensors import randn_, tensor_

from torchjd.autojac._utils import get_leaf_tensors


def test_simple_get_leaf_tensors():
    """Tests that _get_leaf_tensors works correctly in a very simple setting."""

    a1 = tensor_([1.0, 2.0], requires_grad=True)
    a2 = tensor_([3.0, 4.0], requires_grad=True)

    y1 = tensor_([-1.0, 1.0]) @ a1 + a2.sum()
    y2 = (a1**2).sum() + a2.norm()

    leaves = get_leaf_tensors(tensors=[y1, y2], excluded=set())
    assert set(leaves) == {a1, a2}


def test_get_leaf_tensors_excluded_1():
    """
    Tests that _get_leaf_tensors works correctly when some tensors are excluded from the search.

    Note that `a2` itself is not in `excluded`, but it is not accessible from `y1` or `y2` when `b2`
    is excluded from the graph traversal.
    """

    a1 = tensor_([1.0, 2.0], requires_grad=True)
    a2 = tensor_([3.0, 4.0], requires_grad=True)

    b1 = (a1**2).sum()
    b2 = (a2**2).sum()

    y1 = tensor_([-1.0, 1.0]) @ a1 + b2
    y2 = b1

    leaves = get_leaf_tensors(tensors=[y1, y2], excluded={b1, b2})
    assert set(leaves) == {a1}


def test_get_leaf_tensors_excluded_2():
    """
    Tests that _get_leaf_tensors works correctly when some tensors are excluded from the search.

    Even though `b1` and `b2`, that have `a1` and `a2` as descendants, are excluded, `y1` depends on
    `a1` and `a2` from another path, so these two tensors should be in the result.
    """

    a1 = tensor_([1.0, 2.0], requires_grad=True)
    a2 = tensor_([3.0, 4.0], requires_grad=True)

    b1 = (a1**2).sum()
    b2 = (a2**2).sum()

    y1 = tensor_([-1.0, 1.0]) @ a1 + a2.sum()
    y2 = b1

    leaves = get_leaf_tensors(tensors=[y1, y2], excluded={b1, b2})
    assert set(leaves) == {a1, a2}


def test_get_leaf_tensors_leaf_not_requiring_grad():
    """
    Tests that _get_leaf_tensors does not include tensors that do not require grad in its results.
    """

    a1 = tensor_([1.0, 2.0], requires_grad=True)
    a2 = tensor_([3.0, 4.0], requires_grad=False)

    y1 = tensor_([-1.0, 1.0]) @ a1 + a2.sum()
    y2 = (a1**2).sum() + a2.norm()

    leaves = get_leaf_tensors(tensors=[y1, y2], excluded=set())
    assert set(leaves) == {a1}


def test_get_leaf_tensors_model():
    """
    Tests that _get_leaf_tensors works correctly when the autograd graph is generated by a simple
    sequential model.
    """

    x = randn_(16, 10)
    y = randn_(16, 1)

    model = Sequential(Linear(10, 5), ReLU(), Linear(5, 1)).to(device=DEVICE, dtype=DTYPE)
    loss_fn = MSELoss(reduction="none")

    y_hat = model(x)
    losses = loss_fn(y_hat, y)

    leaves = get_leaf_tensors(tensors=[losses], excluded=set())
    assert set(leaves) == set(model.parameters())


def test_get_leaf_tensors_model_excluded_2():
    """
    Tests that _get_leaf_tensors works correctly when the autograd graph is generated by a simple
    sequential model, and some intermediate values are excluded.
    """

    x = randn_(16, 10)
    z = randn_(16, 1)

    model1 = Sequential(Linear(10, 5), ReLU()).to(device=DEVICE, dtype=DTYPE)
    model2 = Linear(5, 1).to(device=DEVICE, dtype=DTYPE)
    loss_fn = MSELoss(reduction="none")

    y = model1(x)
    z_hat = model2(y)
    losses = loss_fn(z_hat, z)

    leaves = get_leaf_tensors(tensors=[losses], excluded={y})
    assert set(leaves) == set(model2.parameters())


def test_get_leaf_tensors_single_root():
    """Tests that _get_leaf_tensors returns no leaves when roots is the empty set."""

    p = tensor_([1.0, 2.0], requires_grad=True)
    y = p * 2

    leaves = get_leaf_tensors(tensors=[y], excluded=set())
    assert set(leaves) == {p}


def test_get_leaf_tensors_empty_roots():
    """Tests that _get_leaf_tensors returns no leaves when roots is the empty set."""

    leaves = get_leaf_tensors(tensors=[], excluded=set())
    assert set(leaves) == set()


def test_get_leaf_tensors_excluded_root():
    """Tests that _get_leaf_tensors correctly excludes the root."""

    a1 = tensor_([1.0, 2.0], requires_grad=True)
    a2 = tensor_([3.0, 4.0], requires_grad=True)

    y1 = tensor_([-1.0, 1.0]) @ a1 + a2.sum()
    y2 = (a1**2).sum()

    leaves = get_leaf_tensors(tensors=[y1, y2], excluded={y1})
    assert set(leaves) == {a1}


@mark.parametrize("depth", [100, 1000, 10000])
def test_get_leaf_tensors_deep(depth: int):
    """Tests that _get_leaf_tensors works when the graph is very deep."""

    one = tensor_(1.0, requires_grad=True)
    sum_ = tensor_(0.0, requires_grad=False)
    for _ in range(depth):
        sum_ = sum_ + one

    leaves = get_leaf_tensors(tensors=[sum_], excluded=set())
    assert set(leaves) == {one}


def test_get_leaf_tensors_leaf():
    """Tests that _get_leaf_tensors raises an error some of the provided tensors are leaves."""

    a = tensor_(1.0, requires_grad=True)
    with raises(ValueError):
        _ = get_leaf_tensors(tensors=[a], excluded=set())


def test_get_leaf_tensors_tensor_not_requiring_grad():
    """
    Tests that _get_leaf_tensors raises an error some of the provided tensors do not require grad.
    """

    a = tensor_(1.0, requires_grad=False) * 2
    with raises(ValueError):
        _ = get_leaf_tensors(tensors=[a], excluded=set())


def test_get_leaf_tensors_excluded_leaf():
    """Tests that _get_leaf_tensors raises an error some of the excluded tensors are leaves."""

    a = tensor_(1.0, requires_grad=True) * 2
    b = tensor_(2.0, requires_grad=True)
    with raises(ValueError):
        _ = get_leaf_tensors(tensors=[a], excluded={b})


def test_get_leaf_tensors_excluded_not_requiring_grad():
    """
    Tests that _get_leaf_tensors raises an error some of the excluded tensors do not require grad.
    """

    a = tensor_(1.0, requires_grad=True) * 2
    b = tensor_(2.0, requires_grad=False) * 2
    with raises(ValueError):
        _ = get_leaf_tensors(tensors=[a], excluded={b})
