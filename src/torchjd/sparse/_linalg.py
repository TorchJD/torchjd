import torch
from torch import Tensor

# TODO: Implement in C everything in this file.


def extended_gcd(a: int, b: int) -> tuple[int, int, int]:
    """
    Extended Euclidean Algorithm (Python integers).
    Returns (g, x, y) such that a*x + b*y = g.
    """
    # We perform the logic in standard Python int for speed on scalars
    # then cast back to torch tensors if needed, or return python ints.
    if a == 0:
        return b, 0, 1
    else:
        g, y, x = extended_gcd(b % a, a)
        return g, x - (b // a) * y, y


def _get_hermite_factor_rank(H: Tensor) -> int:
    """
    Computes the rank of a hermit factor matrix.
    """
    col_magnitudes = torch.sum(torch.abs(H), dim=0)
    return torch.count_nonzero(col_magnitudes).item()


def hnf_decomposition(A: Tensor, reduced: bool) -> tuple[Tensor, Tensor, Tensor]:
    """
    Computes the reduced Hermite Normal Form decomposition using PyTorch. For a matrix A (m x n)
    computes the matrices H (m x r), U (n x r) and V (r x n) such that
        V U = I_r
        A = H V
        H = A U
    where r is the rank of A if reduced is True, and otherwise r is n. In the later case, this also
    satisfies U V = I.

    Args:
        A: (m x n) torch.Tensor (dtype=torch.long)
        reduced: Reduce to rank if True.

    Returns:
        H: (m x r) Canonical Lower Triangular HNF
        U: (n x r) Unimodular transform (A @ U = H)
        V: (r x n) Right inverse Unimodular transform (H @ V = A)
    """

    H = A.clone()
    m, n = H.shape

    U = torch.eye(n, dtype=A.dtype)
    V = torch.eye(n, dtype=A.dtype)

    col = 0

    for row in range(m):
        if n <= col:
            break
        row_slice = H[row, col:n]
        nonzero_indices = torch.nonzero(row_slice)

        if nonzero_indices.numel() > 0:
            relative_pivot_idx = nonzero_indices[0][0].item()
            pivot_idx = col + relative_pivot_idx
        else:
            continue

        if pivot_idx != col:
            H[:, [col, pivot_idx]] = H[:, [pivot_idx, col]]
            U[:, [col, pivot_idx]] = U[:, [pivot_idx, col]]
            V[[col, pivot_idx], :] = V[[pivot_idx, col], :]

        for j in range(col + 1, n):
            if H[row, j] != 0:
                a_val = H[row, col].item()
                b_val = H[row, j].item()

                g, x, y = extended_gcd(a_val, b_val)

                c1 = -b_val // g
                c2 = a_val // g

                H_col = H[:, col]
                H_j = H[:, j]

                H[:, [col, j]] = torch.stack([H_col * x + H_j * y, H_col * c1 + H_j * c2], dim=1)

                U_col = U[:, col]
                U_j = U[:, j]
                U[:, [col, j]] = torch.stack([U_col * x + U_j * y, U_col * c1 + U_j * c2], dim=1)

                V_row_c = V[col, :]
                V_row_j = V[j, :]
                V[[col, j], :] = torch.stack(
                    [V_row_c * c2 - V_row_j * c1, V_row_c * (-y) + V_row_j * x], dim=0
                )

        pivot_val = H[row, col]

        if pivot_val != 0:
            H_row_prefix = H[row, 0:col]
            factors = torch.div(H_row_prefix, pivot_val, rounding_mode="floor")
            H[:, 0:col] -= factors.unsqueeze(0) * H[:, col].unsqueeze(1)
            U[:, 0:col] -= factors.unsqueeze(0) * U[:, col].unsqueeze(1)
            V[col, :] += factors @ V[0:col, :]

        col += 1

    # TODO: Should actually make 2 functions, one for full and one for reduced
    if reduced:
        rank = _get_hermite_factor_rank(H)

        H = H[:, :rank]
        U = U[:, :rank]
        V = V[:rank, :]

    return H, U, V


def compute_gcd(S1: Tensor, S2: Tensor) -> tuple[Tensor, Tensor, Tensor]:
    """
    Computes a GCD and the projection factors, i.e.
    S1 = G @ K1
    S2 = G @ K2
    with G having minimal rank r.

    Args:
        S1, S2: torch.Tensors (m x n1), (m x n2)

    Returns:
        G: (m x r) The Matrix GCD (Canonical Base)
        K1: (r x n1) Factors for S1
        K2: (r x n2) Factors for S2

    Implementation logic:
    The concatenated matrix [S1 | S2] spans exactly the sum of the lattices generated by S1 and S2.
    This is because S1 @ u1 + S2 @ u2 = [S1 | S2] @ [u1.T | u2.T].T
    The reduced HNF decomposition of [S1 | S2] yields G, U, V where the G.shape[1] is the rank of
    [S1 | S2] and [S1 | S2] = G @ V. This means that
    S1 = G @ V[:, :m1]
    S2 = G @ V[:, m1:]
    This is the target common factorization. It is the greatest as the lattice spanned by G is the
    same as that spanned by [S1 | S2].
    """
    assert S1.shape[0] == S2.shape[0]
    m, n1 = S1.shape

    A = torch.cat([S1, S2], dim=1)
    G, _, V = hnf_decomposition(A, True)

    K1 = V[:, :n1]
    K2 = V[:, n1:]

    return G, K1, K2


def compute_lcm(S1: Tensor, S2: Tensor) -> tuple[Tensor, Tensor, Tensor]:
    """
    Computes a LCM and the projection multipliers, i.e.
    L = S1 @ M1 = S2 @ M2
    with L having maximal rank r.

    Args:
        S1, S2: torch.Tensors (m x n1), (m x n2)

    Returns:
        L:  (m x r)  The Matrix LCM
        M1: (n1 x r) Multiplier for S1
        M2: (n2 x r) Multiplier for S2

    Implementation logic:
    The lattice kernel of the concatenated matrix [S1 | -S2] is the set of all vectors
    [u1.T | u2.T].T such that S1 @ u1 - S2 @ u2 = 0, or equivalently S1 @ u1 = S2 @ u2.
    This means that the image of the components of the kernel through S1 and S2 respectively are the
    same which is exactly the intersection of the lattices generated by S1 and S2.
    The full HNF decomposition of [S1 | -S2] yields H, U, V such that
    H = [S1 | -S2] @ U
    If [S1 | -S2] has rank r', then every column of H after the first r' contain only zeros, and
    therefore U[:, r':] spans the kernel of [S1 | -S2]. We have
    S1 @ U[:n1, r':] = S2 @ U[n1:, r':]
    which yields the desired decomposition with r=n1+n2-r'.
    """
    assert S1.shape[0] == S2.shape[0]
    m, n1 = S1.shape

    B = torch.cat([S1, -S2], dim=1)
    H, U, _ = hnf_decomposition(B, False)

    rank = _get_hermite_factor_rank(H)
    M2 = U[n1:, -rank:]
    M1 = U[:n1, -rank:]
    L = S1 @ M1
    return L, M1, M2
