import torch
from torch import Tensor

# TODO: Implement in C everything in this file.


def solve_int(A: Tensor, B: Tensor, tol=1e-9) -> Tensor | None:
    """
    Solve A X = B where A, B and X have integer dtype.
    Return X if such a matrix exists and otherwise None.
    """

    A_ = A.to(torch.float64)
    B_ = B.to(torch.float64)

    try:
        X = torch.linalg.solve(A_, B_)
    except RuntimeError:
        return None

    X_rounded = X.round()
    if not torch.all(torch.isclose(X, X_rounded, atol=tol)):
        return None

    # TODO: Verify that the round operation cannot fail
    return X_rounded.to(torch.int64)


def extended_gcd(a: int, b: int) -> tuple[int, int, int]:
    """
    Extended Euclidean Algorithm (Python integers).
    Returns (g, x, y) such that a*x + b*y = g.
    """
    # We perform the logic in standard Python int for speed on scalars
    # then cast back to torch tensors if needed, or return python ints.
    if a == 0:
        return b, 0, 1
    else:
        g, y, x = extended_gcd(b % a, a)
        return g, x - (b // a) * y, y


def hnf_decomposition(A: Tensor, reduced: bool) -> tuple[Tensor, Tensor, Tensor]:
    """
    Computes the reduced Hermite Normal Form decomposition using PyTorch. For a matrix A (m x n)
    computes the matrices H (m x r), U (n x r) and V (r x n) such that
        V U = I_r
        A = H V
        H = A U
    where r is the rank of A if reduced is True, and otherwise r is n.

    Args:
        A: (m x n) torch.Tensor (dtype=torch.long)
        reduced: Reduce to rank if True.

    Returns:
        H: (m x r) Canonical Lower Triangular HNF
        U: (n x r) Unimodular transform (A @ U = H)
        V: (r x n) Right inverse Unimodular transform (H @ V = A)
    """

    H = A.clone()
    m, n = H.shape

    U = torch.eye(n, dtype=A.dtype)
    V = torch.eye(n, dtype=A.dtype)

    col = 0

    for row in range(m):
        if n <= col:
            break
        row_slice = H[row, col:n]
        nonzero_indices = torch.nonzero(row_slice)

        if nonzero_indices.numel() > 0:
            relative_pivot_idx = nonzero_indices[0][0].item()
            pivot_idx = col + relative_pivot_idx
        else:
            continue

        if pivot_idx != col:
            H[:, [col, pivot_idx]] = H[:, [pivot_idx, col]]
            U[:, [col, pivot_idx]] = U[:, [pivot_idx, col]]
            V[[col, pivot_idx], :] = V[[pivot_idx, col], :]

        for j in range(col + 1, n):
            if H[row, j] != 0:
                a_val = H[row, col].item()
                b_val = H[row, j].item()

                g, x, y = extended_gcd(a_val, b_val)

                c1 = -b_val // g
                c2 = a_val // g

                H_col = H[:, col]
                H_j = H[:, j]

                H[:, [col, j]] = torch.stack([H_col * x + H_j * y, H_col * c1 + H_j * c2], dim=1)

                U_col = U[:, col]
                U_j = U[:, j]
                U[:, [col, j]] = torch.stack([U_col * x + U_j * y, U_col * c1 + U_j * c2], dim=1)

                V_row_c = V[col, :]
                V_row_j = V[j, :]
                V[[col, j], :] = torch.stack(
                    [V_row_c * c2 - V_row_j * c1, V_row_c * (-y) + V_row_j * x], dim=0
                )

        pivot_val = H[row, col]

        if pivot_val != 0:
            H_row_prefix = H[row, 0:col]
            factors = torch.div(H_row_prefix, pivot_val, rounding_mode="floor")
            H[:, 0:col] -= factors.unsqueeze(0) * H[:, col].unsqueeze(1)
            U[:, 0:col] -= factors.unsqueeze(0) * U[:, col].unsqueeze(1)
            V[col, :] += factors @ V[0:col, :]

        col += 1

    if reduced:
        col_magnitudes = torch.sum(torch.abs(H), dim=0)
        rank = torch.count_nonzero(col_magnitudes).item()

        H = H[:, :rank]
        U = U[:, :rank]
        V = V[:rank, :]

    return H, U, V


def compute_gcd(S1: Tensor, S2: Tensor) -> tuple[Tensor, Tensor, Tensor]:
    """
    Computes the GCD and the projection factors. i.e.
    S1 = G @ K1
    S2 = G @ K2
    with G having minimal rank.

    Implementation logic:
    The concatenated matrix [S1 | S2] spans exactly the sum of the lattices generated by S1 and S2.
    This is because S1 @ u1 + S2 @ u2 = [S1 | S2] @ [u1.T | u2.T].T
    The reduced HNF decomposition of [S1 | S2] yields G, U, V where the G.shape[1] is the rank of
    [S1 | S2] and [S1 | S2] = G @ V. This means that
    S1 = G @ V[:, :m1]
    S2 = G @ V[:, m1:]
    This is the target common factorization. It is the greatest as the lattice spanned by G is the
    same as that spanned by [S1 | S2].

    Args:
        S1, S2: torch.Tensors (m x n1), (m x n2)

    Returns:
        G: (m x r) The Matrix GCD (Canonical Base)
        K1: (r x n1) Factors for S1
        K2: (r x n2) Factors for S2
    """
    assert S1.shape[0] == S2.shape[0], "Virtual dimension mismatch"
    m, n1 = S1.shape

    A = torch.cat([S1, S2], dim=1)
    G, _, V = hnf_decomposition(A, True)

    K1 = V[:, :n1]
    K2 = V[:, n1:]

    return G, K1, K2


def compute_lcm(S1, S2):
    """
    Computes the Matrix LCM (L) and the Multiples (M1, M2), i.e.
    L = S1 @ M1 = S2 @ M2

    Returns:
        L:  (m x m)  The Matrix LCM
        M1: (n1 x m) Factor such that L = S1 @ M1
        M2: (n2 x m) Factor such that L = S2 @ M2
    """
    m = S1.shape[0]
    n1 = S1.shape[1]

    # 1. Kernel Setup: [S1 | -S2]
    B = torch.cat([S1, -S2], dim=1)
    H_B, U_B, _ = hnf_decomposition(B, False)

    # 3. Find Zero Columns in H_B (Kernel basis)
    # Sum abs values down columns
    col_mags = torch.sum(torch.abs(H_B), dim=0)
    zero_indices = torch.nonzero(col_mags == 0, as_tuple=True)[0]

    if len(zero_indices) == 0:
        return torch.zeros((m, m), dtype=torch.long)

    # 4. Extract Kernel Basis
    # U_B columns corresponding to H_B zeros are the kernel generators
    kernel_basis = U_B[:, zero_indices]

    # 5. Map back to Image Space
    # The kernel vector is [u; v]. We need u (top n1 rows).
    # Intersection = S1 @ u
    u_parts = kernel_basis[:n1, :]
    L_generators = S1 @ u_parts

    # 6. Canonicalize L
    # The generators might be redundant or non-square.
    # Run HNF one last time to get the unique square LCM matrix.
    L, _, _ = hnf_decomposition(L_generators, False)

    return L[:, :m]
